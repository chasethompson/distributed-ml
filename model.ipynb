{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "import pyspark\n",
    "import pyspark.ml\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import wrangle\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the .randomSplit method to split the 311 data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wrangle.py] reading case.csv\n",
      "[wrangle.py] handling data types\n",
      "[wrangle.py] parsing dates\n",
      "[wrangle.py] adding features\n",
      "[wrangle.py] joining departments\n"
     ]
    }
   ],
   "source": [
    "df = wrangle.wrangle_311(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data into train, test\n",
    "\n",
    "train, test = df.randomSplit([0.8, 0.2], 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(df: pyspark.sql.DataFrame):\n",
    "    return df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(673349, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168355, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 2018-01-01 00:42:00  \n",
      " case_closed_date     | 2018-01-01 12:29:00  \n",
      " case_due_date        | 2020-09-26 00:42:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | true                 \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  EL PASO ST,... \n",
      " council_district     | 005                  \n",
      " num_weeks_late       | -142.6441088         \n",
      " zipcode              | 78207                \n",
      " case_age             | 219                  \n",
      " days_to_closed       | 0                    \n",
      " case_lifetime        | 0                    \n",
      " department           | Animal Care Services \n",
      " dept_subject_to_SLA  | true                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve 'isnan(`case_opened_date`)' due to data type mismatch: argument 1 requires (double or float) type, however, '`case_opened_date`' is of timestamp type.;;\\n'Aggregate [count(CASE WHEN (isnan(cast(case_id#10 as double)) || isnull(case_id#10)) THEN case_id END) AS case_id#4009L, count(CASE WHEN (isnan(case_opened_date#98) || isnull(case_opened_date#98)) THEN case_opened_date END) AS case_opened_date#4011, count(CASE WHEN (isnan(case_closed_date#113) || isnull(case_closed_date#113)) THEN case_closed_date END) AS case_closed_date#4013, count(CASE WHEN (isnan(case_due_date#128) || isnull(case_due_date#128)) THEN case_due_date END) AS case_due_date#4015, count(CASE WHEN (isnan(case_late#68) || isnull(case_late#68)) THEN case_late END) AS case_late#4017, count(CASE WHEN (isnan(num_days_late#15) || isnull(num_days_late#15)) THEN num_days_late END) AS num_days_late#4019L, count(CASE WHEN (isnan(case_closed#53) || isnull(case_closed#53)) THEN case_closed END) AS case_closed#4021, count(CASE WHEN (isnan(cast(service_request_type#18 as double)) || isnull(service_request_type#18)) THEN service_request_type END) AS service_request_type#4023L, count(CASE WHEN (isnan(SLA_days#19) || isnull(SLA_days#19)) THEN SLA_days END) AS SLA_days#4025L, count(CASE WHEN (isnan(cast(case_status#20 as double)) || isnull(case_status#20)) THEN case_status END) AS case_status#4027L, count(CASE WHEN (isnan(cast(source_id#21 as double)) || isnull(source_id#21)) THEN source_id END) AS source_id#4029L, count(CASE WHEN (isnan(cast(request_address#22 as double)) || isnull(request_address#22)) THEN request_address END) AS request_address#4031L, count(CASE WHEN (isnan(cast(council_district#166 as double)) || isnull(council_district#166)) THEN council_district END) AS council_district#4033L, count(CASE WHEN (isnan(num_weeks_late#150) || isnull(num_weeks_late#150)) THEN num_weeks_late END) AS num_weeks_late#4035L, count(CASE WHEN (isnan(cast(zipcode#182 as double)) || isnull(zipcode#182)) THEN zipcode END) AS zipcode#4037L, count(CASE WHEN (isnan(cast(case_age#199 as double)) || isnull(case_age#199)) THEN case_age END) AS case_age#4039L, count(CASE WHEN (isnan(cast(days_to_closed#217 as double)) || isnull(days_to_closed#217)) THEN days_to_closed END) AS days_to_closed#4041L, count(CASE WHEN (isnan(cast(case_lifetime#236 as double)) || isnull(case_lifetime#236)) THEN case_lifetime END) AS case_lifetime#4043L, count(CASE WHEN (isnan(cast(department#359 as double)) || isnull(department#359)) THEN department END) AS department#4045L, count(CASE WHEN (isnan(dept_subject_to_SLA#380) || isnull(dept_subject_to_SLA#380)) THEN dept_subject_to_SLA END) AS dept_subject_to_SLA#4047]\\n+- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, department#359, (dept_subject_to_SLA#269 = YES) AS dept_subject_to_SLA#380]\\n   +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, standardized_dept_name#268 AS department#359, dept_subject_to_SLA#269]\\n      +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, standardized_dept_name#268, dept_subject_to_SLA#269]\\n         +- Project [dept_division#17, case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, standardized_dept_name#268, dept_subject_to_SLA#269]\\n            +- Project [dept_division#17, case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, dept_name#267, standardized_dept_name#268, dept_subject_to_SLA#269]\\n               +- Project [dept_division#17, case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, dept_name#267, standardized_dept_name#268, dept_subject_to_SLA#269]\\n                  +- Join LeftOuter, (dept_division#17 = dept_division#266)\\n                     :- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, CASE WHEN NOT case_closed#53 THEN case_age#199 ELSE days_to_closed#217 END AS case_lifetime#236]\\n                     :  +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, datediff(cast(case_closed_date#113 as date), cast(case_opened_date#98 as date)) AS days_to_closed#217]\\n                     :     +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, datediff(cast(1533742680000000 as date), cast(case_opened_date#98 as date)) AS case_age#199]\\n                     :        +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, regexp_extract(request_address#22, \\\\d+$, 0) AS zipcode#182]\\n                     :           +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, format_string(%03d, cast(council_district#83 as int)) AS council_district#166, num_weeks_late#150]\\n                     :              +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83, (num_days_late#15 / cast(7 as double)) AS num_weeks_late#150]\\n                     :                 +- Project [case_id#10, case_opened_date#98, case_closed_date#113, to_timestamp('case_due_date, Some(M/d/yy H:mm)) AS case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83]\\n                     :                    +- Project [case_id#10, case_opened_date#98, to_timestamp('case_closed_date, Some(M/d/yy H:mm)) AS case_closed_date#113, case_due_date#38, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83]\\n                     :                       +- Project [case_id#10, to_timestamp('case_opened_date, Some(M/d/yy H:mm)) AS case_opened_date#98, case_closed_date#12, case_due_date#38, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83]\\n                     :                          +- Project [case_id#10, case_opened_date#11, case_closed_date#12, case_due_date#38, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, cast(council_district#23 as string) AS council_district#83]\\n                     :                             +- Project [case_id#10, case_opened_date#11, case_closed_date#12, case_due_date#38, (case_late#14 = YES) AS case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#23]\\n                     :                                +- Project [case_id#10, case_opened_date#11, case_closed_date#12, case_due_date#38, case_late#14, num_days_late#15, (case_closed#16 = YES) AS case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#23]\\n                     :                                   +- Project [case_id#10, case_opened_date#11, case_closed_date#12, SLA_due_date#13 AS case_due_date#38, case_late#14, num_days_late#15, case_closed#16, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#23]\\n                     :                                      +- Relation[case_id#10,case_opened_date#11,case_closed_date#12,SLA_due_date#13,case_late#14,num_days_late#15,case_closed#16,dept_division#17,service_request_type#18,SLA_days#19,case_status#20,source_id#21,request_address#22,council_district#23] csv\\n                     +- Relation[dept_division#266,dept_name#267,standardized_dept_name#268,dept_subject_to_SLA#269] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o97.select.\n: org.apache.spark.sql.AnalysisException: cannot resolve 'isnan(`case_opened_date`)' due to data type mismatch: argument 1 requires (double or float) type, however, '`case_opened_date`' is of timestamp type.;;\n'Aggregate [count(CASE WHEN (isnan(cast(case_id#10 as double)) || isnull(case_id#10)) THEN case_id END) AS case_id#4009L, count(CASE WHEN (isnan(case_opened_date#98) || isnull(case_opened_date#98)) THEN case_opened_date END) AS case_opened_date#4011, count(CASE WHEN (isnan(case_closed_date#113) || isnull(case_closed_date#113)) THEN case_closed_date END) AS case_closed_date#4013, count(CASE WHEN (isnan(case_due_date#128) || isnull(case_due_date#128)) THEN case_due_date END) AS case_due_date#4015, count(CASE WHEN (isnan(case_late#68) || isnull(case_late#68)) THEN case_late END) AS case_late#4017, count(CASE WHEN (isnan(num_days_late#15) || isnull(num_days_late#15)) THEN num_days_late END) AS num_days_late#4019L, count(CASE WHEN (isnan(case_closed#53) || isnull(case_closed#53)) THEN case_closed END) AS case_closed#4021, count(CASE WHEN (isnan(cast(service_request_type#18 as double)) || isnull(service_request_type#18)) THEN service_request_type END) AS service_request_type#4023L, count(CASE WHEN (isnan(SLA_days#19) || isnull(SLA_days#19)) THEN SLA_days END) AS SLA_days#4025L, count(CASE WHEN (isnan(cast(case_status#20 as double)) || isnull(case_status#20)) THEN case_status END) AS case_status#4027L, count(CASE WHEN (isnan(cast(source_id#21 as double)) || isnull(source_id#21)) THEN source_id END) AS source_id#4029L, count(CASE WHEN (isnan(cast(request_address#22 as double)) || isnull(request_address#22)) THEN request_address END) AS request_address#4031L, count(CASE WHEN (isnan(cast(council_district#166 as double)) || isnull(council_district#166)) THEN council_district END) AS council_district#4033L, count(CASE WHEN (isnan(num_weeks_late#150) || isnull(num_weeks_late#150)) THEN num_weeks_late END) AS num_weeks_late#4035L, count(CASE WHEN (isnan(cast(zipcode#182 as double)) || isnull(zipcode#182)) THEN zipcode END) AS zipcode#4037L, count(CASE WHEN (isnan(cast(case_age#199 as double)) || isnull(case_age#199)) THEN case_age END) AS case_age#4039L, count(CASE WHEN (isnan(cast(days_to_closed#217 as double)) || isnull(days_to_closed#217)) THEN days_to_closed END) AS days_to_closed#4041L, count(CASE WHEN (isnan(cast(case_lifetime#236 as double)) || isnull(case_lifetime#236)) THEN case_lifetime END) AS case_lifetime#4043L, count(CASE WHEN (isnan(cast(department#359 as double)) || isnull(department#359)) THEN department END) AS department#4045L, count(CASE WHEN (isnan(dept_subject_to_SLA#380) || isnull(dept_subject_to_SLA#380)) THEN dept_subject_to_SLA END) AS dept_subject_to_SLA#4047]\n+- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, department#359, (dept_subject_to_SLA#269 = YES) AS dept_subject_to_SLA#380]\n   +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, standardized_dept_name#268 AS department#359, dept_subject_to_SLA#269]\n      +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, standardized_dept_name#268, dept_subject_to_SLA#269]\n         +- Project [dept_division#17, case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, standardized_dept_name#268, dept_subject_to_SLA#269]\n            +- Project [dept_division#17, case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, dept_name#267, standardized_dept_name#268, dept_subject_to_SLA#269]\n               +- Project [dept_division#17, case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, dept_name#267, standardized_dept_name#268, dept_subject_to_SLA#269]\n                  +- Join LeftOuter, (dept_division#17 = dept_division#266)\n                     :- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, CASE WHEN NOT case_closed#53 THEN case_age#199 ELSE days_to_closed#217 END AS case_lifetime#236]\n                     :  +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, datediff(cast(case_closed_date#113 as date), cast(case_opened_date#98 as date)) AS days_to_closed#217]\n                     :     +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, datediff(cast(1533742680000000 as date), cast(case_opened_date#98 as date)) AS case_age#199]\n                     :        +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, regexp_extract(request_address#22, \\d+$, 0) AS zipcode#182]\n                     :           +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, format_string(%03d, cast(council_district#83 as int)) AS council_district#166, num_weeks_late#150]\n                     :              +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83, (num_days_late#15 / cast(7 as double)) AS num_weeks_late#150]\n                     :                 +- Project [case_id#10, case_opened_date#98, case_closed_date#113, to_timestamp('case_due_date, Some(M/d/yy H:mm)) AS case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83]\n                     :                    +- Project [case_id#10, case_opened_date#98, to_timestamp('case_closed_date, Some(M/d/yy H:mm)) AS case_closed_date#113, case_due_date#38, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83]\n                     :                       +- Project [case_id#10, to_timestamp('case_opened_date, Some(M/d/yy H:mm)) AS case_opened_date#98, case_closed_date#12, case_due_date#38, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83]\n                     :                          +- Project [case_id#10, case_opened_date#11, case_closed_date#12, case_due_date#38, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, cast(council_district#23 as string) AS council_district#83]\n                     :                             +- Project [case_id#10, case_opened_date#11, case_closed_date#12, case_due_date#38, (case_late#14 = YES) AS case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#23]\n                     :                                +- Project [case_id#10, case_opened_date#11, case_closed_date#12, case_due_date#38, case_late#14, num_days_late#15, (case_closed#16 = YES) AS case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#23]\n                     :                                   +- Project [case_id#10, case_opened_date#11, case_closed_date#12, SLA_due_date#13 AS case_due_date#38, case_late#14, num_days_late#15, case_closed#16, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#23]\n                     :                                      +- Relation[case_id#10,case_opened_date#11,case_closed_date#12,SLA_due_date#13,case_late#14,num_days_late#15,case_closed#16,dept_division#17,service_request_type#18,SLA_days#19,case_status#20,source_id#21,request_address#22,council_district#23] csv\n                     +- Relation[dept_division#266,dept_name#267,standardized_dept_name#268,dept_subject_to_SLA#269] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:116)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:356)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:356)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$mapChild$2(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4$$anonfun$apply$13.apply(TreeNode.scala:356)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:296)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:356)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1340)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e58c221fea86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark_df_nan_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-e58c221fea86>\u001b[0m in \u001b[0;36mspark_df_nan_count\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspark_df_nan_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspark_df_nan_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve 'isnan(`case_opened_date`)' due to data type mismatch: argument 1 requires (double or float) type, however, '`case_opened_date`' is of timestamp type.;;\\n'Aggregate [count(CASE WHEN (isnan(cast(case_id#10 as double)) || isnull(case_id#10)) THEN case_id END) AS case_id#4009L, count(CASE WHEN (isnan(case_opened_date#98) || isnull(case_opened_date#98)) THEN case_opened_date END) AS case_opened_date#4011, count(CASE WHEN (isnan(case_closed_date#113) || isnull(case_closed_date#113)) THEN case_closed_date END) AS case_closed_date#4013, count(CASE WHEN (isnan(case_due_date#128) || isnull(case_due_date#128)) THEN case_due_date END) AS case_due_date#4015, count(CASE WHEN (isnan(case_late#68) || isnull(case_late#68)) THEN case_late END) AS case_late#4017, count(CASE WHEN (isnan(num_days_late#15) || isnull(num_days_late#15)) THEN num_days_late END) AS num_days_late#4019L, count(CASE WHEN (isnan(case_closed#53) || isnull(case_closed#53)) THEN case_closed END) AS case_closed#4021, count(CASE WHEN (isnan(cast(service_request_type#18 as double)) || isnull(service_request_type#18)) THEN service_request_type END) AS service_request_type#4023L, count(CASE WHEN (isnan(SLA_days#19) || isnull(SLA_days#19)) THEN SLA_days END) AS SLA_days#4025L, count(CASE WHEN (isnan(cast(case_status#20 as double)) || isnull(case_status#20)) THEN case_status END) AS case_status#4027L, count(CASE WHEN (isnan(cast(source_id#21 as double)) || isnull(source_id#21)) THEN source_id END) AS source_id#4029L, count(CASE WHEN (isnan(cast(request_address#22 as double)) || isnull(request_address#22)) THEN request_address END) AS request_address#4031L, count(CASE WHEN (isnan(cast(council_district#166 as double)) || isnull(council_district#166)) THEN council_district END) AS council_district#4033L, count(CASE WHEN (isnan(num_weeks_late#150) || isnull(num_weeks_late#150)) THEN num_weeks_late END) AS num_weeks_late#4035L, count(CASE WHEN (isnan(cast(zipcode#182 as double)) || isnull(zipcode#182)) THEN zipcode END) AS zipcode#4037L, count(CASE WHEN (isnan(cast(case_age#199 as double)) || isnull(case_age#199)) THEN case_age END) AS case_age#4039L, count(CASE WHEN (isnan(cast(days_to_closed#217 as double)) || isnull(days_to_closed#217)) THEN days_to_closed END) AS days_to_closed#4041L, count(CASE WHEN (isnan(cast(case_lifetime#236 as double)) || isnull(case_lifetime#236)) THEN case_lifetime END) AS case_lifetime#4043L, count(CASE WHEN (isnan(cast(department#359 as double)) || isnull(department#359)) THEN department END) AS department#4045L, count(CASE WHEN (isnan(dept_subject_to_SLA#380) || isnull(dept_subject_to_SLA#380)) THEN dept_subject_to_SLA END) AS dept_subject_to_SLA#4047]\\n+- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, department#359, (dept_subject_to_SLA#269 = YES) AS dept_subject_to_SLA#380]\\n   +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, standardized_dept_name#268 AS department#359, dept_subject_to_SLA#269]\\n      +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, standardized_dept_name#268, dept_subject_to_SLA#269]\\n         +- Project [dept_division#17, case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, standardized_dept_name#268, dept_subject_to_SLA#269]\\n            +- Project [dept_division#17, case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, dept_name#267, standardized_dept_name#268, dept_subject_to_SLA#269]\\n               +- Project [dept_division#17, case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, case_lifetime#236, dept_name#267, standardized_dept_name#268, dept_subject_to_SLA#269]\\n                  +- Join LeftOuter, (dept_division#17 = dept_division#266)\\n                     :- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, days_to_closed#217, CASE WHEN NOT case_closed#53 THEN case_age#199 ELSE days_to_closed#217 END AS case_lifetime#236]\\n                     :  +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, case_age#199, datediff(cast(case_closed_date#113 as date), cast(case_opened_date#98 as date)) AS days_to_closed#217]\\n                     :     +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, zipcode#182, datediff(cast(1533742680000000 as date), cast(case_opened_date#98 as date)) AS case_age#199]\\n                     :        +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#166, num_weeks_late#150, regexp_extract(request_address#22, \\\\d+$, 0) AS zipcode#182]\\n                     :           +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, format_string(%03d, cast(council_district#83 as int)) AS council_district#166, num_weeks_late#150]\\n                     :              +- Project [case_id#10, case_opened_date#98, case_closed_date#113, case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83, (num_days_late#15 / cast(7 as double)) AS num_weeks_late#150]\\n                     :                 +- Project [case_id#10, case_opened_date#98, case_closed_date#113, to_timestamp('case_due_date, Some(M/d/yy H:mm)) AS case_due_date#128, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83]\\n                     :                    +- Project [case_id#10, case_opened_date#98, to_timestamp('case_closed_date, Some(M/d/yy H:mm)) AS case_closed_date#113, case_due_date#38, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83]\\n                     :                       +- Project [case_id#10, to_timestamp('case_opened_date, Some(M/d/yy H:mm)) AS case_opened_date#98, case_closed_date#12, case_due_date#38, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#83]\\n                     :                          +- Project [case_id#10, case_opened_date#11, case_closed_date#12, case_due_date#38, case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, cast(council_district#23 as string) AS council_district#83]\\n                     :                             +- Project [case_id#10, case_opened_date#11, case_closed_date#12, case_due_date#38, (case_late#14 = YES) AS case_late#68, num_days_late#15, case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#23]\\n                     :                                +- Project [case_id#10, case_opened_date#11, case_closed_date#12, case_due_date#38, case_late#14, num_days_late#15, (case_closed#16 = YES) AS case_closed#53, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#23]\\n                     :                                   +- Project [case_id#10, case_opened_date#11, case_closed_date#12, SLA_due_date#13 AS case_due_date#38, case_late#14, num_days_late#15, case_closed#16, dept_division#17, service_request_type#18, SLA_days#19, case_status#20, source_id#21, request_address#22, council_district#23]\\n                     :                                      +- Relation[case_id#10,case_opened_date#11,case_closed_date#12,SLA_due_date#13,case_late#14,num_days_late#15,case_closed#16,dept_division#17,service_request_type#18,SLA_days#19,case_status#20,source_id#21,request_address#22,council_district#23] csv\\n                     +- Relation[dept_division#266,dept_name#267,standardized_dept_name#268,dept_subject_to_SLA#269] csv\\n\""
     ]
    }
   ],
   "source": [
    "def spark_df_nan_count(df):\n",
    "    return df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "\n",
    "spark_df_nan_count(df).show(vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a classification model to predict whether a case will be late or not (i.e. predict case_late). Experiment with different combinations of features and different classification algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 2018-01-01 00:42:00  \n",
      " case_closed_date     | 2018-01-01 12:29:00  \n",
      " case_due_date        | 2020-09-26 00:42:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | true                 \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  EL PASO ST,... \n",
      " council_district     | 005                  \n",
      " num_weeks_late       | -142.6441088         \n",
      " zipcode              | 78207                \n",
      " case_age             | 219                  \n",
      " days_to_closed       | 0                    \n",
      " case_lifetime        | 0                    \n",
      " department           | Animal Care Services \n",
      " dept_subject_to_SLA  | true                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('case_id', 'int'),\n",
       " ('case_opened_date', 'timestamp'),\n",
       " ('case_closed_date', 'timestamp'),\n",
       " ('case_due_date', 'timestamp'),\n",
       " ('case_late', 'boolean'),\n",
       " ('num_days_late', 'double'),\n",
       " ('case_closed', 'boolean'),\n",
       " ('service_request_type', 'string'),\n",
       " ('SLA_days', 'double'),\n",
       " ('case_status', 'string'),\n",
       " ('source_id', 'string'),\n",
       " ('request_address', 'string'),\n",
       " ('council_district', 'string'),\n",
       " ('num_weeks_late', 'double'),\n",
       " ('zipcode', 'string'),\n",
       " ('case_age', 'int'),\n",
       " ('days_to_closed', 'int'),\n",
       " ('case_lifetime', 'int'),\n",
       " ('department', 'string'),\n",
       " ('dept_subject_to_SLA', 'boolean')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using department and council_district first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+------------------+-----------+---------+--------------------+----------------+--------------------+-------+--------+--------------+-------------+--------------------+-------------------+--------------------+-----+\n",
      "|   case_id|   case_opened_date|   case_closed_date|      case_due_date|case_late|      num_days_late|case_closed|service_request_type|          SLA_days|case_status|source_id|     request_address|council_district|      num_weeks_late|zipcode|case_age|days_to_closed|case_lifetime|          department|dept_subject_to_SLA|            features|label|\n",
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+------------------+-----------+---------+--------------------+----------------+--------------------+-------+--------+--------------+-------------+--------------------+-------------------+--------------------+-----+\n",
      "|1014127332|2018-01-01 00:42:00|2018-01-01 12:29:00|2020-09-26 00:42:00|    false| -998.5087616000001|       true|        Stray Animal|             999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|             005|        -142.6441088|  78207|     219|             0|            0|Animal Care Services|               true|(17,[2,8],[1.0,1.0])|  0.0|\n",
      "|1014127333|2018-01-01 00:46:00|2018-01-03 08:11:00|2018-01-05 08:30:00|    false|-2.0126041669999997|       true|Removal Of Obstru...|       4.322222222|     Closed| svcCRMSS|2215  GOLIAD RD, ...|             003|-0.28751488099999994|  78223|     219|             2|            2|Trans & Cap Impro...|               true|(17,[3,10],[1.0,1...|  0.0|\n",
      "|1014127334|2018-01-01 00:48:00|2018-01-02 07:57:00|2018-01-05 08:30:00|    false|       -3.022337963|       true|Removal Of Obstru...|       4.320729167|     Closed| svcCRMSS|102  PALFREY ST W...|             003|-0.43176256614285713|  78223|     219|             1|            1|Trans & Cap Impro...|               true|(17,[3,10],[1.0,1...|  0.0|\n",
      "|1014127335|2018-01-01 01:29:00|2018-01-02 08:13:00|2018-01-17 08:30:00|    false|       -15.01148148|       true|Front Or Side Yar...|       16.29188657|     Closed| svcCRMSS|114  LA GARDE ST,...|             003| -2.1444973542857144|  78223|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,10],[1.0,1...|  0.0|\n",
      "|1014127337|2018-01-01 06:28:00|2018-01-01 14:38:00|2018-01-31 08:30:00|    false|       -29.74398148|       true|Traffic Signal Op...|       30.08446759|     Closed| svcCRMSS|BANDERA RD and BR...|             007|  -4.249140211428571|       |     219|             0|            0|Trans & Cap Impro...|               true|(17,[3,13],[1.0,1...|  0.0|\n",
      "|1014127338|2018-01-01 06:57:00|2018-01-02 15:32:00|2018-01-17 08:30:00|    false|       -14.70673611|       true|Front Or Side Yar...|       16.06429398|     Closed| svcCRMSS|10133  FIGARO CAN...|             004| -2.1009623014285714|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127339|2018-01-01 06:58:00|2018-01-02 15:32:00|2018-01-17 08:30:00|    false|       -14.70662037|       true|Front Or Side Yar...|16.063796300000003|     Closed| svcCRMSS|10133  FIGARO CAN...|             004|  -2.100945767142857|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127340|2018-01-01 06:58:00|2018-01-02 15:32:00|2018-01-17 08:30:00|    false|       -14.70662037|       true|Right Of Way/Side...|       16.06333333|     Closed| svcCRMSS|10133  FIGARO CAN...|             004|  -2.100945767142857|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127342|2018-01-01 07:00:00|2018-01-02 15:32:00|2018-01-17 08:30:00|    false|       -14.70649306|       true|Front Or Side Yar...|       16.06237269|     Closed| svcCRMSS|10133  FIGARO CAN...|             004|         -2.10092758|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127346|2018-01-01 07:04:00|2018-01-02 15:33:00|2018-01-17 08:30:00|    false|       -14.70623843|       true|Right Of Way/Side...|       16.05953704|     Closed| svcCRMSS|10129  BOXING PAS...|             004| -2.1008912042857144|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127347|2018-01-01 07:04:00|2018-01-02 15:33:00|2018-01-17 08:30:00|    false|-14.705891199999998|       true|Front Or Side Yar...|       16.05907407|     Closed| svcCRMSS|834  BARREL POINT...|             004|          -2.1008416|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127348|2018-01-01 07:05:00|2018-01-02 15:33:00|2018-01-17 08:30:00|    false|       -14.70600694|       true|Front Or Side Yar...|       16.05864583|     Closed| svcCRMSS|834  BARREL POINT...|             004|  -2.100858134285714|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127349|2018-01-01 07:06:00|2018-01-02 15:33:00|2018-01-17 08:30:00|    false|       -14.70576389|       true|Right Of Way/Side...|       16.05819444|     Closed| svcCRMSS|834  BARREL POINT...|             004|  -2.100823412857143|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127350|2018-01-01 07:06:00|2018-01-02 15:33:00|2018-01-17 08:30:00|    false|       -14.70576389|       true|Front Or Side Yar...|       16.05775463|     Closed| svcCRMSS|834  BARREL POINT...|             004|  -2.100823412857143|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127351|2018-01-01 07:07:00|2018-01-02 15:33:00|2018-01-17 08:30:00|    false|       -14.70564815|       true|Front Or Side Yar...|       16.05733796|     Closed| svcCRMSS|834  BARREL POINT...|             004| -2.1008068785714284|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127352|2018-01-01 07:08:00|2018-01-02 15:34:00|2018-01-17 08:30:00|    false|       -14.70540509|       true|Right Of Way/Side...|       16.05690972|     Closed| svcCRMSS|834  BARREL POINT...|             004| -2.1007721557142855|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127353|2018-01-01 07:08:00|2018-01-02 15:34:00|2018-01-17 08:30:00|    false|       -14.70552083|       true|Front Or Side Yar...|       16.05643519|     Closed| svcCRMSS|834  BARREL POINT...|             004|         -2.10078869|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127354|2018-01-01 07:09:00|2018-01-02 15:34:00|2018-01-17 08:30:00|    false|       -14.70494213|       true|Front Or Side Yar...|       16.05594907|     Closed| svcCRMSS|834  BARREL POINT...|             004| -2.1007060185714286|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127355|2018-01-01 07:10:00|2018-01-02 15:35:00|2018-01-17 08:30:00|    false|       -14.70481481|       true|Right Of Way/Side...|       16.05549769|     Closed| svcCRMSS|834  BARREL POINT...|             004|         -2.10068783|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "|1014127356|2018-01-01 07:10:00|2018-01-02 15:35:00|2018-01-17 08:30:00|    false|       -14.70481481|       true|Front Or Side Yar...|            16.055|     Closed| svcCRMSS|837  BARREL POINT...|             004|         -2.10068783|  78251|     219|             1|            1|DSD/Code Enforcement|               true|(17,[0,11],[1.0,1...|  0.0|\n",
      "+----------+-------------------+-------------------+-------------------+---------+-------------------+-----------+--------------------+------------------+-----------+---------+--------------------+----------------+--------------------+-------+--------+--------------+-------------+--------------------+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = pyspark.ml.feature.RFormula(formula='case_late ~ department + council_district').fit(train)\n",
    "train_input = rf.transform(train)\n",
    "train_input.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pyspark.ml.classification.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_fit = lr.fit(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6360295499748985"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6369172734209141"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = pyspark.ml.evaluation.BinaryClassificationEvaluator()\n",
    "test_auc = evaluator.evaluate(lr_fit.transform(rf.transform(test)))\n",
    "test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = rf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|prediction|   0.0|  1.0|\n",
      "+----------+------+-----+\n",
      "|       0.0|149412|18402|\n",
      "|       1.0|   149|  392|\n",
      "+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(lr_fit.transform(test_input)\n",
    " .select('case_late', 'department', 'council_district', 'label', 'probability', 'prediction')\n",
    " .groupby('prediction') # predicted == rows\n",
    " .pivot('label') # actual values are columns\n",
    " .count()\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+---------+------------------+-----------+--------------------+--------+-----------+---------+--------------------+----------------+--------------+-------+--------+--------------+-------------+--------------------+-------------------+--------------------+-----+\n",
      "|   case_id|   case_opened_date|   case_closed_date|      case_due_date|case_late|     num_days_late|case_closed|service_request_type|SLA_days|case_status|source_id|     request_address|council_district|num_weeks_late|zipcode|case_age|days_to_closed|case_lifetime|          department|dept_subject_to_SLA|            features|label|\n",
      "+----------+-------------------+-------------------+-------------------+---------+------------------+-----------+--------------------+--------+-----------+---------+--------------------+----------------+--------------+-------+--------+--------------+-------------+--------------------+-------------------+--------------------+-----+\n",
      "|1014127332|2018-01-01 00:42:00|2018-01-01 12:29:00|2020-09-26 00:42:00|    false|-998.5087616000001|       true|        Stray Animal|   999.0|     Closed| svcCRMLS|2315  EL PASO ST,...|             005|  -142.6441088|  78207|     219|             0|            0|Animal Care Services|               true|(143,[2,14],[1.0,...|  0.0|\n",
      "+----------+-------------------+-------------------+-------------------+---------+------------------+-----------+--------------------+--------+-----------+---------+--------------------+----------------+--------------+-------+--------+--------------+-------------+--------------------+-------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = pyspark.ml.feature.RFormula(formula='case_late ~ department + source_id').fit(train)\n",
    "train_input = rf.transform(train)\n",
    "train_input.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = pyspark.ml.classification.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_fit = lr.fit(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6368578325642402"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_fit.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = pyspark.ml.evaluation.BinaryClassificationEvaluator()\n",
    "# test_auc = evaluator.evaluate(lr_fit.transform(rf.transform(test)))\n",
    "# test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o817.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 238.0 failed 1 times, most recent failure: Lost task 0.0 in stage 238.0 (TID 1282, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: 141954.  To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:260)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:151)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:226)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: 141954.  To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:260)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 17 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-583e8d0ce015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_auc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/ml/evaluation.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \"\"\"\n\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0misLargerBetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o817.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 238.0 failed 1 times, most recent failure: Lost task 0.0 in stage 238.0 (TID 1282, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: 141954.  To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:260)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:309)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:151)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n\tat org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4$lzycompute(BinaryClassificationMetrics.scala:155)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$4(BinaryClassificationMetrics.scala:146)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:148)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:226)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:86)\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:97)\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:87)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: 141954.  To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:260)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "test_auc = evaluator.evaluate(lr_fit.transform(rf.transform(test)))\n",
    "test_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a regression model to predict how many days late a case will be (i.e. predict num_days_late). Experiment with different combinations of features and different regression algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
